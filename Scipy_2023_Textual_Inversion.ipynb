{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "RDtS_pBu-UWD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWNpWKw5nCzx",
        "outputId": "1bf0bf37-8287-40ba-d968-c8c44c61fbbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-cv (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/keras-team/keras-cv.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "import keras_cv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras_cv import layers as cv_layers\n",
        "from keras_cv.models.stable_diffusion import NoiseScheduler\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "stable_diffusion = keras_cv.models.StableDiffusion()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWMonsNI9c1Y",
        "outputId": "c6782147-a839-4714-b6f5-b5a80b2de137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Functions"
      ],
      "metadata": {
        "id": "h0qQr-tm-Wff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_images(images):\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    for i in range(len(images)):\n",
        "        ax = plt.subplot(1, len(images), i + 1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.axis(\"off\")\n"
      ],
      "metadata": {
        "id": "sbA7eYoI-ZGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assemble datasets"
      ],
      "metadata": {
        "id": "FphOfVNdhBpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def assemble_image_dataset(urls):\n",
        "    # Fetch all remote files\n",
        "    files = [tf.keras.utils.get_file(origin=url) for url in urls]\n",
        "\n",
        "    # Resize images\n",
        "    resize = keras.layers.Resizing(height=512, width=512, crop_to_aspect_ratio=True)\n",
        "    images = [keras.utils.load_img(img) for img in files]\n",
        "    images = [keras.utils.img_to_array(img) for img in images]\n",
        "    images = np.array([resize(img) for img in images])\n",
        "\n",
        "    # The StableDiffusion image encoder requires images to be normalized to the\n",
        "    # [-1, 1] pixel value range\n",
        "    images = images / 127.5 - 1\n",
        "\n",
        "    # Create the tf.data.Dataset\n",
        "    image_dataset = tf.data.Dataset.from_tensor_slices(images)\n",
        "\n",
        "    # Shuffle and introduce random noise\n",
        "    image_dataset = image_dataset.shuffle(50, reshuffle_each_iteration=True)\n",
        "    image_dataset = image_dataset.map(\n",
        "        cv_layers.RandomCropAndResize(\n",
        "            target_size=(512, 512),\n",
        "            crop_area_factor=(0.8, 1.0),\n",
        "            aspect_ratio_factor=(1.0, 1.0),\n",
        "        ),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    )\n",
        "    image_dataset = image_dataset.map(\n",
        "        cv_layers.RandomFlip(mode=\"horizontal\"),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    )\n",
        "    return image_dataset\n"
      ],
      "metadata": {
        "id": "O7j5vn6nhAZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_PROMPT_LENGTH = 77\n",
        "placeholder_token = \"<my-funny-cat-token>\"\n",
        "\n",
        "\n",
        "def pad_embedding(embedding):\n",
        "    return embedding + (\n",
        "        [stable_diffusion.tokenizer.end_of_text] * (MAX_PROMPT_LENGTH - len(embedding))\n",
        "    )\n",
        "\n",
        "\n",
        "stable_diffusion.tokenizer.add_tokens(placeholder_token)\n",
        "\n",
        "\n",
        "def assemble_text_dataset(prompts):\n",
        "    prompts = [prompt.format(placeholder_token) for prompt in prompts]\n",
        "    embeddings = [stable_diffusion.tokenizer.encode(prompt) for prompt in prompts]\n",
        "    embeddings = [np.array(pad_embedding(embedding)) for embedding in embeddings]\n",
        "    text_dataset = tf.data.Dataset.from_tensor_slices(embeddings)\n",
        "    text_dataset = text_dataset.shuffle(100, reshuffle_each_iteration=True)\n",
        "    return text_dataset\n"
      ],
      "metadata": {
        "id": "Gtab8S6MhMjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def assemble_dataset(urls, prompts):\n",
        "    image_dataset = assemble_image_dataset(urls)\n",
        "    text_dataset = assemble_text_dataset(prompts)\n",
        "    # the image dataset is quite short, so we repeat it to match the length of the\n",
        "    # text prompt dataset\n",
        "    image_dataset = image_dataset.repeat()\n",
        "    # we use the text prompt dataset to determine the length of the dataset.  Due to\n",
        "    # the fact that there are relatively few prompts we repeat the dataset 5 times.\n",
        "    # we have found that this anecdotally improves results.\n",
        "    text_dataset = text_dataset.repeat(5)\n",
        "    return tf.data.Dataset.zip((image_dataset, text_dataset))\n"
      ],
      "metadata": {
        "id": "h7RjR3KxhPXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assembling a text-image pair dataset\n",
        "\n",
        "In order to train the embedding of our new token, we first must assemble a dataset\n",
        "consisting of text-image pairs.\n",
        "Each sample from the dataset must contain an image of the concept we are teaching\n",
        "StableDiffusion, as well as a caption accurately representing the content of the image.\n",
        "\n",
        "Feel free to use your custom image and prompts. For this example we will be using some cat doll images"
      ],
      "metadata": {
        "id": "yEXniurA_WZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "single_ds = assemble_dataset(\n",
        "    urls=[\n",
        "        \"https://i.imgur.com/VIedH1X.jpg\",\n",
        "        \"https://i.imgur.com/eBw13hE.png\",\n",
        "        \"https://i.imgur.com/oJ3rSg7.png\",\n",
        "        \"https://i.imgur.com/5mCL6Df.jpg\",\n",
        "        \"https://i.imgur.com/4Q6WWyI.jpg\",\n",
        "    ],\n",
        "    prompts=[\n",
        "        \"a photo of a {}\",\n",
        "        \"a rendering of a {}\",\n",
        "        \"a cropped photo of the {}\",\n",
        "        \"the photo of a {}\",\n",
        "        \"a photo of a clean {}\",\n",
        "        \"a photo of my {}\",\n",
        "        \"a photo of the cool {}\",\n",
        "        \"a close-up photo of a {}\",\n",
        "        \"a bright photo of the {}\",\n",
        "        \"a cropped photo of a {}\",\n",
        "        \"a photo of the {}\",\n",
        "        \"a good photo of the {}\",\n",
        "        \"a photo of one {}\",\n",
        "        \"a close-up photo of the {}\",\n",
        "        \"a rendition of the {}\",\n",
        "        \"a photo of the clean {}\",\n",
        "        \"a rendition of a {}\",\n",
        "        \"a photo of a nice {}\",\n",
        "        \"a good photo of a {}\",\n",
        "        \"a photo of the nice {}\",\n",
        "        \"a photo of the small {}\",\n",
        "        \"a photo of the weird {}\",\n",
        "        \"a photo of the large {}\",\n",
        "        \"a photo of a cool {}\",\n",
        "        \"a photo of a small {}\",\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "cSH6UR4kina_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![https://i.imgur.com/gQCRjK6.png](https://i.imgur.com/gQCRjK6.png)\n",
        "\n",
        "Looks great!\n",
        "\n",
        "Next, we assemble a dataset of groups of our GitHub avatars:"
      ],
      "metadata": {
        "id": "s7rjecwPi0HK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group_ds = assemble_dataset(\n",
        "    urls=[\n",
        "        \"https://i.imgur.com/yVmZ2Qa.jpg\",\n",
        "        \"https://i.imgur.com/JbyFbZJ.jpg\",\n",
        "        \"https://i.imgur.com/CCubd3q.jpg\",\n",
        "    ],\n",
        "    prompts=[\n",
        "        \"a photo of a group of {}\",\n",
        "        \"a rendering of a group of {}\",\n",
        "        \"a cropped photo of the group of {}\",\n",
        "        \"the photo of a group of {}\",\n",
        "        \"a photo of a clean group of {}\",\n",
        "        \"a photo of my group of {}\",\n",
        "        \"a photo of a cool group of {}\",\n",
        "        \"a close-up photo of a group of {}\",\n",
        "        \"a bright photo of the group of {}\",\n",
        "        \"a cropped photo of a group of {}\",\n",
        "        \"a photo of the group of {}\",\n",
        "        \"a good photo of the group of {}\",\n",
        "        \"a photo of one group of {}\",\n",
        "        \"a close-up photo of the group of {}\",\n",
        "        \"a rendition of the group of {}\",\n",
        "        \"a photo of the clean group of {}\",\n",
        "        \"a rendition of a group of {}\",\n",
        "        \"a photo of a nice group of {}\",\n",
        "        \"a good photo of a group of {}\",\n",
        "        \"a photo of the nice group of {}\",\n",
        "        \"a photo of the small group of {}\",\n",
        "        \"a photo of the weird group of {}\",\n",
        "        \"a photo of the large group of {}\",\n",
        "        \"a photo of a cool group of {}\",\n",
        "        \"a photo of a small group of {}\",\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "IBdxf3Czit0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![https://i.imgur.com/GY9Pf3D.png](https://i.imgur.com/GY9Pf3D.png)\n",
        "\n",
        "Finally, we concatenate the two datasets:"
      ],
      "metadata": {
        "id": "2Wc3yM--i2Tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = single_ds.concatenate(group_ds)\n",
        "train_ds = train_ds.batch(1).shuffle(\n",
        "    train_ds.cardinality(), reshuffle_each_iteration=True\n",
        ")"
      ],
      "metadata": {
        "id": "5YFwYLLRi9Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding a new token to the text encoder"
      ],
      "metadata": {
        "id": "thCd1HgRjARJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get text encoding for your new token\n",
        "tokenized_initializer = stable_diffusion.tokenizer.encode(\"cat\")[1]\n",
        "\n",
        "# Generate embedding for the new token\n",
        "new_weights = stable_diffusion.text_encoder.layers[2].token_embedding(\n",
        "    tf.constant(tokenized_initializer)\n",
        ")\n",
        "\n",
        "# Get len of .vocab instead of tokenizer\n",
        "new_vocab_size = len(stable_diffusion.tokenizer.vocab)\n",
        "\n",
        "# The embedding layer is the 2nd layer in the text encoder\n",
        "old_token_weights = stable_diffusion.text_encoder.layers[\n",
        "    2\n",
        "].token_embedding.get_weights()\n",
        "old_position_weights = stable_diffusion.text_encoder.layers[\n",
        "    2\n",
        "].position_embedding.get_weights()\n",
        "\n",
        "# Genrate new embedding weights by concatenating the old weights with the new embedding vector\n",
        "old_token_weights = old_token_weights[0]\n",
        "new_weights = np.expand_dims(new_weights, axis=0)\n",
        "new_weights = np.concatenate([old_token_weights, new_weights], axis=0)"
      ],
      "metadata": {
        "id": "RDJvSXDqjEZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Have to set download_weights False so we can init (otherwise tries to load weights)\n",
        "new_encoder = keras_cv.models.stable_diffusion.TextEncoder(\n",
        "    keras_cv.models.stable_diffusion.stable_diffusion.MAX_PROMPT_LENGTH,\n",
        "    vocab_size=new_vocab_size,\n",
        "    download_weights=False,\n",
        ")\n",
        "for index, layer in enumerate(stable_diffusion.text_encoder.layers):\n",
        "    # Layer 2 is the embedding layer, so we omit it from our weight-copying\n",
        "    if index == 2:\n",
        "        continue\n",
        "    new_encoder.layers[index].set_weights(layer.get_weights())\n",
        "\n",
        "\n",
        "new_encoder.layers[2].token_embedding.set_weights([new_weights])\n",
        "new_encoder.layers[2].position_embedding.set_weights(old_position_weights)\n",
        "\n",
        "stable_diffusion._text_encoder = new_encoder\n",
        "stable_diffusion._text_encoder.compile(jit_compile=True)"
      ],
      "metadata": {
        "id": "zhF0bEOlmA_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "b-qDYyxWmOav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In TextualInversion, the only piece of the model that is trained is the embedding vector.\n",
        "Let's freeze the rest of the model."
      ],
      "metadata": {
        "id": "pRPB5L5IoGL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stable_diffusion.diffusion_model.trainable = False\n",
        "stable_diffusion.decoder.trainable = False\n",
        "stable_diffusion.text_encoder.trainable = True\n",
        "\n",
        "stable_diffusion.text_encoder.layers[2].trainable = True\n",
        "\n",
        "\n",
        "def traverse_layers(layer):\n",
        "    if hasattr(layer, \"layers\"):\n",
        "        for layer in layer.layers:\n",
        "            yield layer\n",
        "    if hasattr(layer, \"token_embedding\"):\n",
        "        yield layer.token_embedding\n",
        "    if hasattr(layer, \"position_embedding\"):\n",
        "        yield layer.position_embedding\n",
        "\n",
        "\n",
        "for layer in traverse_layers(stable_diffusion.text_encoder):\n",
        "    if isinstance(layer, keras.layers.Embedding) or \"clip_embedding\" in layer.name:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "\n",
        "new_encoder.layers[2].position_embedding.trainable = False"
      ],
      "metadata": {
        "id": "ZkOPcgLTmOF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's confirm the proper weights are set to trainable."
      ],
      "metadata": {
        "id": "zYIJDL7UoI8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_models = [\n",
        "    stable_diffusion.text_encoder,\n",
        "    stable_diffusion.diffusion_model,\n",
        "    stable_diffusion.decoder,\n",
        "]\n",
        "print([[w.shape for w in model.trainable_weights] for model in all_models])"
      ],
      "metadata": {
        "id": "t85MoIqUoBTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the new embedding\n",
        "\n",
        "In order to train the embedding, we need a couple of utilities.\n",
        "We import a NoiseScheduler from KerasCV, and define the following utilities below:\n",
        "\n",
        "- `sample_from_encoder_outputs` is a wrapper around the base StableDiffusion image\n",
        "encoder which samples from the statistical distribution produced by the image\n",
        "encoder, rather than taking just the mean (like many other SD applications)\n",
        "- `get_timestep_embedding` produces an embedding for a specified timestep for the\n",
        "diffusion model\n",
        "- `get_position_ids` produces a tensor of position IDs for the text encoder (which is just a\n",
        "series from `[1, MAX_PROMPT_LENGTH]`)"
      ],
      "metadata": {
        "id": "N9FDsoqVrXX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Remove the top layer from the encoder, which cuts off the variance and only returns\n",
        "# the mean\n",
        "training_image_encoder = keras.Model(\n",
        "    stable_diffusion.image_encoder.input,\n",
        "    stable_diffusion.image_encoder.layers[-2].output,\n",
        ")\n",
        "\n",
        "\n",
        "def sample_from_encoder_outputs(outputs):\n",
        "    mean, logvar = tf.split(outputs, 2, axis=-1)\n",
        "    logvar = tf.clip_by_value(logvar, -30.0, 20.0)\n",
        "    std = tf.exp(0.5 * logvar)\n",
        "    sample = tf.random.normal(tf.shape(mean))\n",
        "    return mean + std * sample\n",
        "\n",
        "\n",
        "def get_timestep_embedding(timestep, dim=320, max_period=10000):\n",
        "    half = dim // 2\n",
        "    freqs = tf.math.exp(\n",
        "        -math.log(max_period) * tf.range(0, half, dtype=tf.float32) / half\n",
        "    )\n",
        "    args = tf.convert_to_tensor([timestep], dtype=tf.float32) * freqs\n",
        "    embedding = tf.concat([tf.math.cos(args), tf.math.sin(args)], 0)\n",
        "    return embedding\n",
        "\n",
        "\n",
        "def get_position_ids():\n",
        "    return tf.convert_to_tensor([list(range(MAX_PROMPT_LENGTH))], dtype=tf.int32)\n"
      ],
      "metadata": {
        "id": "LmkdEVPZrbbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next, we implement a StableDiffusionFineTuner"
      ],
      "metadata": {
        "id": "PZW51wJrrd4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class StableDiffusionFineTuner(keras.Model):\n",
        "    def __init__(self, stable_diffusion, noise_scheduler, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.stable_diffusion = stable_diffusion\n",
        "        self.noise_scheduler = noise_scheduler\n",
        "\n",
        "    def train_step(self, data):\n",
        "        images, embeddings = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Sample from the predicted distribution for the training image\n",
        "            latents = sample_from_encoder_outputs(training_image_encoder(images))\n",
        "            # The latents must be downsampled to match the scale of the latents used\n",
        "            # in the training of StableDiffusion.  This number is truly just a \"magic\"\n",
        "            # constant that they chose when training the model.\n",
        "            latents = latents * 0.18215\n",
        "\n",
        "            # Produce random noise in the same shape as the latent sample\n",
        "            noise = tf.random.normal(tf.shape(latents))\n",
        "            batch_dim = tf.shape(latents)[0]\n",
        "\n",
        "            # Pick a random timestep for each sample in the batch\n",
        "            timesteps = tf.random.uniform(\n",
        "                (batch_dim,),\n",
        "                minval=0,\n",
        "                maxval=noise_scheduler.train_timesteps,\n",
        "                dtype=tf.int64,\n",
        "            )\n",
        "\n",
        "            # Add noise to the latents based on the timestep for each sample\n",
        "            noisy_latents = self.noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "            # Encode the text in the training samples to use as hidden state in the\n",
        "            # diffusion model\n",
        "            encoder_hidden_state = self.stable_diffusion.text_encoder(\n",
        "                [embeddings, get_position_ids()]\n",
        "            )\n",
        "\n",
        "            # Compute timestep embeddings for the randomly-selected timesteps for each\n",
        "            # sample in the batch\n",
        "            timestep_embeddings = tf.map_fn(\n",
        "                fn=get_timestep_embedding,\n",
        "                elems=timesteps,\n",
        "                fn_output_signature=tf.float32,\n",
        "            )\n",
        "\n",
        "            # Call the diffusion model\n",
        "            noise_pred = self.stable_diffusion.diffusion_model(\n",
        "                [noisy_latents, timestep_embeddings, encoder_hidden_state]\n",
        "            )\n",
        "\n",
        "            # Compute the mean-squared error loss and reduce it.\n",
        "            loss = self.compiled_loss(noise_pred, noise)\n",
        "            loss = tf.reduce_mean(loss, axis=2)\n",
        "            loss = tf.reduce_mean(loss, axis=1)\n",
        "            loss = tf.reduce_mean(loss)\n",
        "\n",
        "        # Load the trainable weights and compute the gradients for them\n",
        "        trainable_weights = self.stable_diffusion.text_encoder.trainable_weights\n",
        "        grads = tape.gradient(loss, trainable_weights)\n",
        "\n",
        "        # Gradients are stored in indexed slices, so we have to find the index\n",
        "        # of the slice(s) which contain the placeholder token.\n",
        "        index_of_placeholder_token = tf.reshape(tf.where(grads[0].indices == 49408), ())\n",
        "        condition = grads[0].indices == 49408\n",
        "        condition = tf.expand_dims(condition, axis=-1)\n",
        "\n",
        "        # Override the gradients, zeroing out the gradients for all slices that\n",
        "        # aren't for the placeholder token, effectively freezing the weights for\n",
        "        # all other tokens.\n",
        "        grads[0] = tf.IndexedSlices(\n",
        "            values=tf.where(condition, grads[0].values, 0),\n",
        "            indices=grads[0].indices,\n",
        "            dense_shape=grads[0].dense_shape,\n",
        "        )\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(grads, trainable_weights))\n",
        "        return {\"loss\": loss}\n"
      ],
      "metadata": {
        "id": "t9mdpjpCrjCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we start training, let's take a look at what StableDiffusion produces for our\n",
        "token."
      ],
      "metadata": {
        "id": "T73Ffa4na1Qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated = stable_diffusion.text_to_image(\n",
        "    f\"an oil painting of {placeholder_token}\", seed=1337, batch_size=3\n",
        ")\n",
        "plot_images(generated)"
      ],
      "metadata": {
        "id": "ux7UACwZat8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets start training"
      ],
      "metadata": {
        "id": "WNpSfZcKWi1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_scheduler = NoiseScheduler(\n",
        "    beta_start=0.00085,\n",
        "    beta_end=0.012,\n",
        "    beta_schedule=\"scaled_linear\",\n",
        "    train_timesteps=1000,\n",
        ")\n",
        "trainer = StableDiffusionFineTuner(stable_diffusion, noise_scheduler, name=\"trainer\")\n",
        "EPOCHS = 50\n",
        "learning_rate = keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate=1e-4, decay_steps=train_ds.cardinality() * EPOCHS\n",
        ")\n",
        "optimizer = keras.optimizers.Adam(\n",
        "    weight_decay=0.004, learning_rate=learning_rate, epsilon=1e-8, global_clipnorm=10\n",
        ")\n",
        "\n",
        "trainer.compile(\n",
        "    optimizer=optimizer,\n",
        "    # We are performing reduction manually in our train step, so none is required here.\n",
        "    loss=keras.losses.MeanSquaredError(reduction=\"none\"),\n",
        ")"
      ],
      "metadata": {
        "id": "jxjChBDoa8Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        ")"
      ],
      "metadata": {
        "id": "G-qqXCnja-0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lets try it out"
      ],
      "metadata": {
        "id": "d8ARToLXbWtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated = stable_diffusion.text_to_image(\n",
        "    f\"Gandalf as a {placeholder_token} fantasy art drawn by disney concept artists, \"\n",
        "    \"golden colour, high quality, highly detailed, elegant, sharp focus, concept art, \"\n",
        "    \"character concepts, digital painting, mystery, adventure\",\n",
        "    batch_size=3,\n",
        ")\n",
        "plot_images(generated)"
      ],
      "metadata": {
        "id": "2juwwwFabfTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try out more prompts and check out the results"
      ],
      "metadata": {
        "id": "p6GyFYSBojkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "14cbbUpnood3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}