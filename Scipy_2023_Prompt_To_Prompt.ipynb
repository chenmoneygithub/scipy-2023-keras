{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "eQ3KzWwbato9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58jJHsKGnEJ7",
        "outputId": "dfff9adb-4853-467e-a889-e84822f6d6b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-cv (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/miguelCalado/keras-cv -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pylab as P\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from keras_cv.models.stable_diffusion import StableDiffusion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAMcAviBayS1",
        "outputId": "17125614-1475-41dc-8dd8-cbf115eaa8b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You do not have Waymo Open Dataset installed, so KerasCV Waymo metrics are not available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility fuctions"
      ],
      "metadata": {
        "id": "9YbL7e-dbH8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ShowImage(im, ax=None, save_fig=None):\n",
        "    if ax is None:\n",
        "        P.figure()\n",
        "    P.xticks([])\n",
        "    P.yticks([])\n",
        "    P.imshow(im)\n",
        "    if save_fig:\n",
        "        P.savefig(\n",
        "            save_fig, dpi=200, bbox_inches=\"tight\", pad_inches=0.0, transparent=\"True\"\n",
        "        )"
      ],
      "metadata": {
        "id": "2r4lAXE-bMCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_attention_weights(prompt, attn_weights):\n",
        "        \"\"\"Create an array of weights to scale the attention maps associated with each prompt token.\n",
        "        This is used for manipulating the importance of the prompt tokens,\n",
        "        increasing or decreasing the importance assigned to each word.\n",
        "\n",
        "        Args:\n",
        "            prompt: The prompt string to tokenize, must be 77 tokens or shorter.\n",
        "            attn_weights: A list of tuples containing the\n",
        "                pair of word and weight to be manipulated.\n",
        "\n",
        "        Returns:\n",
        "            weights: Array of weights to control the importance of each prompt token.\n",
        "\n",
        "        Example:\n",
        "\n",
        "        ```python\n",
        "        from keras_cv.models import StableDiffusion\n",
        "\n",
        "        model = StableDiffusion(img_height=512, img_width=512, jit_compile=True)\n",
        "\n",
        "        prompt = \"a fluffy teddy bear\"\n",
        "        prompt_weights = [(\"fluffy\", -4)]\n",
        "        attn_weights = generator.create_attention_weights(prompt, prompt_weights)\n",
        "        ```\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialize the weights to 1.\n",
        "        weights = np.ones(77)\n",
        "\n",
        "        # Get the prompt tokens\n",
        "        tokens = generator.tokenize_prompt(prompt)\n",
        "\n",
        "        # Extract the weights and words\n",
        "        edit_words, edit_weights = zip(*attn_weights)\n",
        "\n",
        "        # Tokenize the words to edit\n",
        "        edit_tokens = [generator.tokenizer.encode(word)[1:-1] for word in edit_words]\n",
        "\n",
        "        # Get the indexes of the tokens\n",
        "        index_edit_tokens = tf.where(tf.equal(tokens, edit_tokens))[:, -1]\n",
        "\n",
        "        # Replace the original weight values\n",
        "        weights[index_edit_tokens] = tf.constant(edit_weights)\n",
        "        return weights"
      ],
      "metadata": {
        "id": "IjNNc6qVbhGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt to prompt editing"
      ],
      "metadata": {
        "id": "uOetRy2_bX0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate an image"
      ],
      "metadata": {
        "id": "bs2euD21cTcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommendation: if you have a low memory gpu drop the batch to 1\n",
        "BATCH_SIZE = 2\n",
        "NUM_STEPS = 50\n",
        "UNCONDITIONAL_GUIDANCE_SCALE = 8\n",
        "\n",
        "# Stable Diffusion 1.x\n",
        "generator = StableDiffusion(\n",
        "    img_height=512,\n",
        "    img_width=512,\n",
        "    jit_compile=False,\n",
        ")\n",
        "\n",
        "# Lets start by generating some chiwawas\n",
        "print(\"Generating pictures of chiwawas\")\n",
        "prompt = \"a photo of a chiwawa with sunglasses\"\n",
        "seed = 1235\n",
        "img_org = generator.text_to_image(\n",
        "    prompt=prompt,\n",
        "    num_steps=NUM_STEPS,\n",
        "    unconditional_guidance_scale=UNCONDITIONAL_GUIDANCE_SCALE,\n",
        "    seed=seed,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "ShowImage(img_org)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bGrTEfEcYIo",
        "outputId": "42634a02-cc01-4104-dfa7-9de61ec254e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n",
            "Generating pictures of chiwawas\n",
            "Downloading data from https://github.com/openai/CLIP/blob/main/clip/bpe_simple_vocab_16e6.txt.gz?raw=true\n",
            "1356917/1356917 [==============================] - 0s 0us/step\n",
            "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_encoder.h5\n",
            "492466864/492466864 [==============================] - 7s 0us/step\n",
            "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_diffusion_model.h5\n",
            "3439090152/3439090152 [==============================] - 53s 0us/step\n",
            "13/50 [======>.......................] - ETA: 49:59"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Swap"
      ],
      "metadata": {
        "id": "8QMmedbwbooJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "self_attn_steps = 0.4\n",
        "cross_attn_steps = 0.6\n",
        "img_edit = generator.prompt_to_prompt(\n",
        "        prompt=\"a photo of a chiwawa with sunglasses\",\n",
        "        prompt_edit=\"a photo of a chiwawa with googles\",\n",
        "        method=\"replace\",\n",
        "        self_attn_steps=self_attn_steps,\n",
        "        cross_attn_steps=cross_attn_steps,\n",
        "        num_steps=NUM_STEPS,\n",
        "        unconditional_guidance_scale=UNCONDITIONAL_GUIDANCE_SCALE,\n",
        "        seed=seed,\n",
        "        batch_size=BATCH_SIZE,\n",
        "    )\n",
        "ShowImage(img_edit)"
      ],
      "metadata": {
        "id": "z_AF_Y3ecP-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt refinement"
      ],
      "metadata": {
        "id": "_sonK_eccDXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "img_edit = generator.prompt_to_prompt(\n",
        "        prompt=\"a photo of a chiwawa with sunglasses\",\n",
        "        prompt_edit=\"a photo of a chiwawa with heart shaped sunglasses\",\n",
        "        method=\"refine\",\n",
        "        self_attn_steps=self_attn_steps,\n",
        "        cross_attn_steps=cross_attn_steps,\n",
        "        num_steps=NUM_STEPS,\n",
        "        unconditional_guidance_scale=UNCONDITIONAL_GUIDANCE_SCALE,\n",
        "        seed=seed,\n",
        "        batch_size=BATCH_SIZE,\n",
        "    )\n",
        "ShowImage(img_edit)"
      ],
      "metadata": {
        "id": "ufAu8kdLcPdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Re-weight"
      ],
      "metadata": {
        "id": "o2k_wM8kcIa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "prompt=\"a fluffy teddy bear\"\n",
        "img_org = generator.text_to_image(\n",
        "    prompt=prompt,\n",
        "    num_steps=NUM_STEPS,\n",
        "    unconditional_guidance_scale=UNCONDITIONAL_GUIDANCE_SCALE,\n",
        "    seed=seed,\n",
        "    batch_size=1,\n",
        ")\n",
        "ShowImage(img_edit)"
      ],
      "metadata": {
        "id": "rFZnSZN0cO8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_weights = [(\"fluffy\", -5)]\n",
        "attn_weights = create_attention_weights(prompt, prompt_weights)\n",
        "\n",
        "self_attn_steps = 0.2\n",
        "attn_edit_weights = 0.6\n",
        "\n",
        "# Clean up the session to avoid clutter from old models and layers\n",
        "tf.keras.backend.clear_session()\n",
        "# Generate Prompt-to-Prompt\n",
        "img_edit = generator.prompt_to_prompt(\n",
        "    prompt=prompt,\n",
        "    prompt_edit=prompt,\n",
        "    method=\"reweight\",\n",
        "    self_attn_steps=self_attn_steps,\n",
        "    cross_attn_steps=attn_edit_weights,\n",
        "    attn_edit_weights=attn_weights,\n",
        "    num_steps=NUM_STEPS,\n",
        "    unconditional_guidance_scale=UNCONDITIONAL_GUIDANCE_SCALE,\n",
        "    seed=seed,\n",
        "    batch_size=1,\n",
        ")"
      ],
      "metadata": {
        "id": "7VzPrADCeVqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## try making the teddy bear more fluffy"
      ],
      "metadata": {
        "id": "7ta1AarEf4-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "PY9a6p95eubE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}